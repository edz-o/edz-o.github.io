<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yi Zhang</title>

    <meta name="author" content="Yi Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/x-icon" href="./images/favicon.ico">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-153053613-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-153053613-1');
    </script>
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Yi Zhang</name>
                                    </p>
                                    <p>I obtained my Ph.D. at Johns Hopkins University, advised by Bloomberg
                                        Distinguished Professor <a href="http://www.cs.jhu.edu/~ayuille/">Alan
                                            Yuille</a>, where I worked on computer vision and machine learning.
                                    </p>
                                    <p>
                                        I received my B.E. in EE from Tsinghua University.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:edwardz.amg@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="./data/cv_yi.pdf">CV</a> &nbsp/&nbsp
                                        <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                                        <a href="https://scholar.google.com/citations?user=8aXDeK8AAAAJ">Google
                                            Scholar</a> &nbsp/&nbsp
                                        <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                                        <a href="https://github.com/edz-o/">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/yi_photo_new.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/yi_photo_circle.png"
                                            class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        My current research focuses on: 1) human analysis 2) generative models for (3D)
                                        computer vision, and 3) robustness and generalization.
                                        <!-- I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>. -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/DIRECT-3D.jpeg" alt="prl" width="160" height="80">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>Learning Direct Text-to-3D Generation on Massive Noisy 3D Data
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://qihao067.github.io/">Qihao Liu</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://songbai.site/">Song Bai</a>,
                                    <a href="https://generativevision.mpi-inf.mpg.de/">Adam Kortylewski</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>CVPR</em>, 2024
                                    <br>
                                    <a href="">paper</a> /
                                    <a href="https://arxiv.org/abs/2406.04322">arXiv</a> /
                                    <a href="https://direct-3d.github.io/">Project</a> /
                                    <a href="https://github.com/qihao067/direct3d">code</a>
                                    <p>Enabling training 3D diffusion model on large-scale noisy 3D data. </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/DST.jpeg" alt="prl" width="140" height="140">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>Generating Images with 3D Annotations using Diffusion Models
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://wufeim.github.io/">Wufei Ma</a>,
                                    <a href="https://qihao067.github.io/">Qihao Liu</a>,
                                    <a href="https://jiahaoplus.github.io/">Jiahao Wang</a>,
                                    <a href="https://www.xiaodingyuan.com/">Xiaoding Yuan</a>,
                                    <a href="https://scholar.google.com/citations?user=YR7re-cAAAAJ">Angtian Wang</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&hl=en">Zihao
                                        Xiao</a>,
                                    <a href="">Guofeng Zhang</a>,
                                    <a href="">Beijia Lu</a>,
                                    <a href="https://scholar.google.com/citations?user=aG-fi1cAAAAJ&hl=en">Ruxiao
                                        Duan</a>,
                                    <a href="">Yongrui Qi</a>,
                                    <a href="https://generativevision.mpi-inf.mpg.de/">Adam Kortylewski</a>,
                                    <a href="https://www.cs.jhu.edu/~yyliu/">Yaoyao Liu</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ICLR</em>, 2024 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
                                    <br>
                                    <a href="">paper</a> /
                                    <a href="https://arxiv.org/abs/2306.08103">arXiv</a> /
                                    <a href="">Project</a> /
                                    <a href="">code</a>
                                    <p>Generating images with 3D Annotations using Diffusion Models. </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/3dnbf.png" alt="prl" width="160" height="80">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://3dnbf.github.io">
                                        <papertitle>3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose
                                            Estimation</papertitle>
                                    </a>
                                    <br>
                                    <strong>Yi Zhang*</strong>,
                                    <a href="https://scholar.google.com/citations?user=g5XaopwAAAAJ">Pengliang Ji*</a>,
                                    <a href="https://scholar.google.com/citations?user=YR7re-cAAAAJ">Angtian Wang</a>,
                                    <a href="https://meijieru.com/">Jieru Mei</a>,
                                    <a href="https://generativevision.mpi-inf.mpg.de/">Adam Kortylewski</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ICCV</em>, 2023
                                    <br>
                                    <a href="data/3dnbf_paper.pdf">paper</a> /
                                    <a href="https://arxiv.org/abs/2308.10123">arXiv</a> /
                                    <a href="https://3dnbf.github.io">Project</a> /
                                    <a href="https://github.com/edz-o/3DNBF">code</a>
                                    <p>An analysis-by-synthesis approach for 3D human pose estimation that is highly
                                        robust to occlusions.</p>
                                </td>
                            </tr>

                            <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/animal3d.png" , height="120" , width="160">
                                </td>
                                <td style="width:75%;vertical-align:middle">
                                    <a href="">
                                        <papertitle>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://xujiacong.github.io/">Jiacong Xu</a>,
                                    <strong>Yi Zhang</strong>, ...,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                                    <a href="https://gvrl.mpi-inf.mpg.de/">Adam Kortylewski</a>
                                    <br>
                                    <em>ICCV</em>, 2023
                                    <br>
                                    <a href="">paper</a> /
                                    <a href="https://arxiv.org/abs/2308.11737">arXiv</a> /
                                    <a href="https://xujiacong.github.io/Animal3D/">Project</a> /
                                    <a href="https://github.com/XuJiacong/Animal3D">code</a>
                                    <p>
                                        Animal3D consists of 3379 images collected from 40 mammal species, high-quality
                                        annotations of 26 keypoints, and importantly the pose and shape
                                        parameters of the SMAL model. We demonstrate that synthetic pre-training is a
                                        viable strategy to boost the model performance.
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/volumehuman.png" alt="prl" width="160" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="">
                <papertitle>Volumetric Neural Human for Robust Pose Optimization via Analysis-by-synthesis</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=g5XaopwAAAAJ">Pengliang Ji</a>,
                <a href="https://scholar.google.com/citations?user=YR7re-cAAAAJ">Angtian Wang</a>, 
                <strong>Yi Zhang</strong>, 
                <a href="https://generativevision.mpi-inf.mpg.de/">Adam Kortylewski</a>, 
                <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>SVRHM 2022 Workshop@ NeurIPS</em>, 2022
                <br>
                <a href="https://openreview.net/pdf?id=FO8alu6qRNW">Paper</a>
                <p>An analysis-by-synthesis approach for 3D human pose estimation.</p>
            </td>
        </tr> -->

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/hupor.jpeg" alt="prl" width="160" height="140">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>Explicit Occlusion Reasoning for Multi-person 3D Human Pose
                                            Estimation</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://qihao067.github.io/">Qihao Liu</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://songbai.site/">Song Bai</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ECCV</em>, 2022
                                    <br>
                                    <a
                                        href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">paper</a>
                                    /
                                    <a href="https://arxiv.org/abs/2208.00090">arXiv</a> /
                                    <a href="https://github.com/qihao067/HUPOR">code</a>
                                    <p>We explicitly reason about occlusion in multi-person 3D human pose estimation
                                        that can generalize better than using pose priors/constraints, data
                                        augmentation, or implicit reasoning.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/daszl.png" alt="prl" width="160" height="160">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>DASZL: Dynamic Action Signatures for Zero-shot Learning</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://taesoo-kim.github.io/">Tae Soo Kim*</a>,
                                    <a href="https://jd-jones.github.io/">Jonathan Jones*</a>,
                                    <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ">Michael Peven*</a>,
                                    <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ">Zihao Xiao</a>,
                                    <a href="https://www.linkedin.com/in/jin-bai-460a51107">Jin Bai</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                                    <a href="https://www.cs.jhu.edu/hager/">Gregory D. Hager</a>
                                    <br>
                                    <em>AAAI</em>, 2021
                                    <br>
                                    <a href="https://arxiv.org/abs/1912.03613">arXiv</a>
                                    <p>This compositional approach allows us to reframe fine-grained recognition as
                                        zero-shot activity recognition, where a detector is composed “on the fly” from
                                        simple first-principles state machines supported by deep-learned components.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/nls.png" alt="prl" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>Nuisance-Label Supervision: Robustness Improvement by Free Labels
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://sites.google.com/view/xinyue-wei">Xinyue Wei</a>,
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ">Zihao Xiao</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ICCVW</em>, 2021
                                    <br>
                                    <a href="https://arxiv.org/abs/2110.07118">arXiv</a> /
                                    <a
                                        href="hhttps://openaccess.thecvf.com/content/ICCV2021W/ILDAV/supplemental/Wei_Nuisance-Label_Supervision_Robustness_ICCVW_2021_supplemental.pdf">supp</a>
                                    /
                                    <a
                                        href="https://drive.google.com/file/d/1ba_jX2bdSL7WWkF0HsTE8OCaowKQQnMP/view?usp=sharing">poster</a>
                                    <p>Improving model robustness to nuisance factors using "free" nuidance labels and
                                        adversarial debiasing.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/synthcp.png" alt="prl" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>Synthesize then Compare: Detecting Failures and Anomalies for
                                            Semantic Segmentation</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://yingdaxia.github.io/">Yingda Xia*</a>,
                                    <strong>Yi Zhang*</strong>,
                                    <a href="https://scholar.google.com/citations?user=T3EjsaAAAAAJ">Fengze Liu</a>,
                                    <a href="https://shenwei1231.github.io/">Wei Shen</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong>
                                    </font>
                                    <br>
                                    <a href="https://arxiv.org/abs/2003.08440">arXiv</a> /
                                    <a href="https://github.com/YingdaXia/SynthCP">code</a> /
                                    <a href="https://youtu.be/YN2qnV7phcU">video</a>
                                    <p>A simple unified framework for failure and anomaly detection for segmantic
                                        segmentation based on a generative model and a comparison module. </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/rsa.png" alt="prl" width="160" height="130">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>RSA: Randomized Simulation as Augmentation for Robust Human Action
                                            Recognition</papertitle>
                                    </a>
                                    <br>
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://sites.google.com/view/xinyue-wei">Xinyue Wei</a>,
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ">Zihao Xiao</a>,
                                    <a href="https://www.cs.jhu.edu/hager/">Gregory D. Hager</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>arXiv</em>, 2019
                                    <br>
                                    <a href="https://arxiv.org/abs/1912.01180">arXiv</a>
                                    <p>We use simulated videos to augment real training data to improve model robustness
                                        to nuisance factors, e.g. novel viewpoint, change
                                        of background and human appearance.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/unrealstereo.jpg" alt="prl" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://stereo.unrealcv.org/">
                                        <papertitle>UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <a href="https://qchenclaire.github.io/">Qi Chen</a>,
                                    <a href="http://www.xlhu.cn/">Xiaolin Hu</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>3DV</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong>
                                    </font>
                                    <br>
                                    <a href="data/yi2018unrealstereo.pdf">paper</a> /
                                    <a href="https://stereo.unrealcv.org/">project page</a> /
                                    <a href="https://github.com/edz-o/unreal-stereo-evaluation">code</a>
                                    <p>We control hazardous factors, e.g. specularity, texturelessness and transparency,
                                        to analyze robustness of binocular stereo algorithms. Findings in virtual world
                                        is verified in real world. </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/sampleahead_teaser.png" alt="prl" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="">
                                        <papertitle>SampleAhead: Online Classifier-Sampler Communication for Learning
                                            from Synthesized Data</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://qchenclaire.github.io/">Qi Chen</a>,
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>BMVC</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong>
                                    </font>
                                    <br>
                                    <a href="https://arxiv.org/abs/1804.00248">arXiv</a>
                                    <p>Efficient online sampling algorithm for active learning from the combinatorially
                                        large synthetic data space.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="images/unrealcv.png" alt="prl" width="160" height="40">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://unrealcv.org/">
                                        <papertitle>UnrealCV: Virtual Worlds for Computer Vision</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                                    <a href="http://fangweizhong.xyz/">Fangwei Zhong</a>,
                                    <strong>Yi Zhang</strong>,
                                    <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ">Zihao Xiao</a>,
                                    <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
                                    <a href="https://taesoo-kim.github.io/">Tae Soo Kim</a>,
                                    <a href="http://www.xlhu.cn/">Yizhou Wang</a>,
                                    <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                                    <br>
                                    <em>ACM Multimedia Open Source Software Competition</em>, 2017
                                    <br>
                                    <a href="data/unrealcv.pdf">paper</a> /
                                    <a href="https://unrealcv.org/">project page</a>
                                    <p>An open-source tool for interacting with and obtain data from virtual worlds in
                                        <a href="https://www.unrealengine.com/">Unreal Engine</a>.
                                    </p>
                                </td>
                            </tr>

                            <!-- <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/zipnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/zipnerf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function zipnerf_start() {
            document.getElementById('zipnerf_image').style.opacity = "1";
          }

          function zipnerf_stop() {
            document.getElementById('zipnerf_image').style.opacity = "0";
          }
          zipnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://jonbarron.info/zipnerf">
          <papertitle>Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</papertitle>
        </a>
        <br>
        <strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="http://jonbarron.info/zipnerf">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
        /
        <a href="https://arxiv.org/abs/TODO">arXiv</a>
        <p></p>
        <p>
        Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;76% and accelerate training by 22x.
        </p>
      </td>
    </tr>
    
		
    <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/owl.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/owl.png' width="160">
        </div>
        <script type="text/javascript">
          function db3d_start() {
            document.getElementById('db3d_image').style.opacity = "1";
          }

          function db3d_stop() {
            document.getElementById('db3d_image').style.opacity = "0";
          }
          db3d_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
				<a href="https://dreambooth3d.github.io/">
          <papertitle>DreamBooth3D: Subject-Driven Text-to-3D Generation</papertitle>
        </a>
        <br>
        
<a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
<a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
         <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
        <br>
        <em>arXiv</em>, 2023
        <br>
				<a href="https://dreambooth3d.github.io/">project page</a> / 
				<a href="https://arxiv.org/abs/2303.13508">arXiv</a>
        <p></p>
        <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
      </td>
    </tr>

    

    <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='alignerf_image'>
            <img src='images/alignerf_after.jpg' width="160"></div>
          <img src='images/alignerf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function alignerf_start() {
            document.getElementById('alignerf_image').style.opacity = "1";
          }

          function alignerf_stop() {
            document.getElementById('alignerf_image').style.opacity = "0";
          }
          alignerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://yifanjiang19.github.io/alignerf">
          <papertitle>AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training
</papertitle>
        </a>
        <br>
        <a href="https://yifanjiang.net/">Yifan Jiang</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>, 
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://ir1d.github.io/">Dejia Xu</a>, <br>
        <strong>Jonathan T. Barron</strong>,
        <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
        <a href="https://tianfan.info/">Tianfan Xue</a>
        <br>
        <em>CVPR</em>, 2023
        <br>
        <a href="https://yifanjiang19.github.io/alignerf">project page</a>
        /
        <a href="https://arxiv.org/abs/2211.09682">arXiv</a>
        <p></p>
        <p>
        Accounting for misalignment due to scene motion or calibration errors improves NeRF reconstruction quality.
        </p>
      </td>
    </tr>
    
<tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
      <source src="images/dreamfusion.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/dreamfusion.jpg' width="160">
    </div>
    <script type="text/javascript">
      function dreamfusion_start() {
        document.getElementById('dreamfusion_image').style.opacity = "1";
      }

      function dreamfusion_stop() {
        document.getElementById('dreamfusion_image').style.opacity = "0";
      }
      dreamfusion_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://dreamfusion3d.github.io/">
      <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>
    </a>
    <br>
    <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
    <a href="https://www.ajayj.com/">Ajay Jain</a>,
    <strong>Jonathan T. Barron</strong>,
		<a href="https://bmild.github.io/">Ben Mildenhall</a>
    <br>
    <em>ICLR</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Outstanding Paper Award)</strong></font>
    <br>
    <a href="https://dreamfusion3d.github.io/">project page</a>
    /
    <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
    /
    <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
    <p></p>
    <p>
    We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling.
    </p>
  </td>
</tr>-->


                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        The website code is borrowed from <a href="https://jonbarron.info/">Jon
                                            Barron</a>'s <a href="https://github.com/jonbarron/jonbarron_website">source
                                            code</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>
